{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11035066,"sourceType":"datasetVersion","datasetId":6873191},{"sourceId":11036050,"sourceType":"datasetVersion","datasetId":6873886},{"sourceId":11081622,"sourceType":"datasetVersion","datasetId":6906844}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nprint(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n\n\nimport pickle\n\n# Path to the file\nfile_path = \"/kaggle/input/preprocessed-mimic-train-test/preprocessed_data_train_test.pkl\"\n\n# Load the data\nwith open(file_path, 'rb') as file:\n    meta_data = pickle.load(file)\n\n# Now you can work with the loaded data\n# Let's see what's inside\nprint(type(meta_data))\nprint(meta_data.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:42:58.957978Z","iopub.execute_input":"2025-03-19T05:42:58.958269Z","iopub.status.idle":"2025-03-19T05:43:22.917851Z","shell.execute_reply.started":"2025-03-19T05:42:58.958239Z","shell.execute_reply":"2025-03-19T05:43:22.916684Z"}},"outputs":[{"name":"stdout","text":"GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n<class 'pandas.core.frame.DataFrame'>\nIndex(['path', 'subject_id', 'study_id', 'dicom_id', 'split', 'gender',\n       'insurance', 'anchor_age', 'race', 'Enlarged Cardiomediastinum',\n       'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation',\n       'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion',\n       'Pleural Other', 'Fracture', 'Support Devices', 'No Finding'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"meta_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:43:22.918800Z","iopub.execute_input":"2025-03-19T05:43:22.919388Z","iopub.status.idle":"2025-03-19T05:43:22.981205Z","shell.execute_reply.started":"2025-03-19T05:43:22.919331Z","shell.execute_reply":"2025-03-19T05:43:22.980236Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                path subject_id  study_id  \\\n0  generalized-image-embeddings-for-the-mimic-che...   10000032  50414267   \n1  generalized-image-embeddings-for-the-mimic-che...   10000032  53189527   \n2  generalized-image-embeddings-for-the-mimic-che...   10000032  53911762   \n3  generalized-image-embeddings-for-the-mimic-che...   10000032  53911762   \n4  generalized-image-embeddings-for-the-mimic-che...   10000032  56699142   \n\n                                       dicom_id  split gender insurance  \\\n0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  train      F  Medicaid   \n1  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  train      F  Medicaid   \n2  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  train      F  Medicaid   \n3  fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818  train      F  Medicaid   \n4  ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c  train      F  Medicaid   \n\n   anchor_age   race  Enlarged Cardiomediastinum  ...  Edema  Consolidation  \\\n0        52.0  WHITE                         0.0  ...    0.0            0.0   \n1        52.0  WHITE                         0.0  ...    0.0            0.0   \n2        52.0  WHITE                         0.0  ...    0.0            0.0   \n3        52.0  WHITE                         0.0  ...    0.0            0.0   \n4        52.0  WHITE                         0.0  ...    0.0            0.0   \n\n   Pneumonia  Atelectasis  Pneumothorax  Pleural Effusion  Pleural Other  \\\n0        0.0          0.0           0.0               0.0            0.0   \n1        0.0          0.0           0.0               0.0            0.0   \n2        0.0          0.0           0.0               0.0            0.0   \n3        0.0          0.0           0.0               0.0            0.0   \n4        0.0          0.0           0.0               0.0            0.0   \n\n   Fracture  Support Devices  No Finding  \n0       0.0              0.0         1.0  \n1       0.0              0.0         1.0  \n2       0.0              0.0         1.0  \n3       0.0              0.0         1.0  \n4       0.0              0.0         1.0  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>subject_id</th>\n      <th>study_id</th>\n      <th>dicom_id</th>\n      <th>split</th>\n      <th>gender</th>\n      <th>insurance</th>\n      <th>anchor_age</th>\n      <th>race</th>\n      <th>Enlarged Cardiomediastinum</th>\n      <th>...</th>\n      <th>Edema</th>\n      <th>Consolidation</th>\n      <th>Pneumonia</th>\n      <th>Atelectasis</th>\n      <th>Pneumothorax</th>\n      <th>Pleural Effusion</th>\n      <th>Pleural Other</th>\n      <th>Fracture</th>\n      <th>Support Devices</th>\n      <th>No Finding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>generalized-image-embeddings-for-the-mimic-che...</td>\n      <td>10000032</td>\n      <td>50414267</td>\n      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n      <td>train</td>\n      <td>F</td>\n      <td>Medicaid</td>\n      <td>52.0</td>\n      <td>WHITE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>generalized-image-embeddings-for-the-mimic-che...</td>\n      <td>10000032</td>\n      <td>53189527</td>\n      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n      <td>train</td>\n      <td>F</td>\n      <td>Medicaid</td>\n      <td>52.0</td>\n      <td>WHITE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>generalized-image-embeddings-for-the-mimic-che...</td>\n      <td>10000032</td>\n      <td>53911762</td>\n      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n      <td>train</td>\n      <td>F</td>\n      <td>Medicaid</td>\n      <td>52.0</td>\n      <td>WHITE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>generalized-image-embeddings-for-the-mimic-che...</td>\n      <td>10000032</td>\n      <td>53911762</td>\n      <td>fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818</td>\n      <td>train</td>\n      <td>F</td>\n      <td>Medicaid</td>\n      <td>52.0</td>\n      <td>WHITE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>generalized-image-embeddings-for-the-mimic-che...</td>\n      <td>10000032</td>\n      <td>56699142</td>\n      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n      <td>train</td>\n      <td>F</td>\n      <td>Medicaid</td>\n      <td>52.0</td>\n      <td>WHITE</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\ntf.get_logger().setLevel(logging.ERROR)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Base path for dataset\nbase_path = \"/kaggle/input/mimic-data/generalized-image-embeddings-for-the-mimic-chest-x-ray-dataset-1.0/\"\n\n# Label columns\nlabel_columns = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n                'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', \n                'Atelectasis', 'Pneumothorax', 'Pleural Effusion', \n                'Pleural Other', 'Fracture', 'Support Devices', 'No Finding']\n\n# TFRecord feature description\nfeature_description = {\n    'embedding': tf.io.FixedLenFeature([1376], tf.float32),\n    'image/id': tf.io.FixedLenFeature([], tf.string),\n    'image/format': tf.io.FixedLenFeature([], tf.string)\n}\n\ndef parse_tfrecord(record):\n    \"\"\"Parse a TFRecord into a dictionary of features\"\"\"\n    example = tf.io.parse_single_example(record, feature_description)\n    return {\n        'embedding': example['embedding'].numpy(),\n        'image_id': example['image/id'].numpy().decode('utf-8')\n    }\n\nclass MIMICEmbeddingDataset(Dataset):\n    def __init__(self, metadata_df, base_path):\n        self.metadata_df = metadata_df  # Your existing metadata DataFrame\n        self.base_path = base_path\n        self.label_columns = label_columns\n        self.data = []\n        \n        # Process each row in the metadata\n        logger.info(\"Processing metadata to connect with embeddings...\")\n        self._load_data()\n        logger.info(f\"Dataset size: {len(self.data)}\")\n    \n    def _load_data(self):\n        skipped_files = 0\n        \n        # Process each row in the metadata\n        for idx, row in tqdm(self.metadata_df.iterrows(), total=len(self.metadata_df), desc=\"Processing metadata\"):\n            # Get the relative path\n            tfrecord_path = os.path.join(self.base_path, row['path'])\n            \n            if not os.path.exists(tfrecord_path):\n                logger.warning(f\"File not found: {tfrecord_path}\")\n                skipped_files += 1\n                continue\n            \n            # Load the TFRecord\n            dataset = tf.data.TFRecordDataset(tfrecord_path)\n            \n            for record in dataset:\n                # Parse the TFRecord\n                parsed = parse_tfrecord(record)\n                embedding = parsed['embedding']\n                \n                # Add to dataset with metadata\n                self.data.append({\n                    'embedding': embedding,\n                    'subject_id': row['subject_id'],\n                    'study_id': row['study_id'],\n                    'labels': {col: row[col] for col in self.label_columns},\n                    'split': row['split'],\n                    'demographics': {\n                        'gender': row['gender'],\n                        'insurance': row['insurance'],\n                        'race': row['race'],\n                        'anchor_age': row['anchor_age']\n                    }\n                })\n                \n                # We only need one record per row\n                break\n        \n        logger.info(f\"Processed {len(self.data)} records. Skipped files: {skipped_files}\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        embedding_tensor = torch.tensor(item['embedding'], dtype=torch.float32)\n        \n        result = {\n            'embedding': embedding_tensor,\n            'subject_id': item['subject_id'],\n            'study_id': item['study_id'],\n            'split': item['split']\n        }\n        \n        # Process labels\n        label_values = []\n        for col in self.label_columns:\n            value = item['labels'].get(col, 0)\n            if pd.isna(value):\n                value = 0\n            label_values.append(float(value))\n        \n        result['labels'] = torch.tensor(label_values, dtype=torch.float32)\n        \n        return result\n\n# Create dataset with your existing meta_data DataFrame\ndataset = MIMICEmbeddingDataset(meta_data, base_path)\n\n# Create train and test datasets based on the split column\ntrain_indices = [i for i, item in enumerate(dataset.data) if item['split'] == 'train']\ntest_indices = [i for i, item in enumerate(dataset.data) if item['split'] == 'test']\n\n# Split the train set into train and validation (90/10)\ntrain_size = int(0.9 * len(train_indices))\nval_size = len(train_indices) - train_size\n\n# Create a random generator with fixed seed for reproducibility\ngenerator = torch.Generator().manual_seed(42)\n\n# Split train indices into train and val\ntrain_indices, val_indices = random_split(train_indices, [train_size, val_size], generator=generator)\n\n# Create subset datasets\nclass SubsetDatasetWithIndices(Dataset):\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n    \n    def __len__(self):\n        return len(self.indices)\n    \n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\ntrain_dataset = SubsetDatasetWithIndices(dataset, train_indices.indices)\nval_dataset = SubsetDatasetWithIndices(dataset, val_indices.indices)\ntest_dataset = SubsetDatasetWithIndices(dataset, test_indices)\n\n# Create data loaders\nbatch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Print dataset sizes\nlogger.info(f\"Total dataset size: {len(dataset)}\")\nlogger.info(f\"Train dataset size: {len(train_dataset)}\")\nlogger.info(f\"Validation dataset size: {len(val_dataset)}\")\nlogger.info(f\"Test dataset size: {len(test_dataset)}\")\n\n# Test the datasets\nif len(train_dataset) > 0:\n    sample = train_dataset[0]\n    logger.info(f\"Sample embedding shape: {sample['embedding'].shape}\")\n    logger.info(f\"Sample labels shape: {sample['labels'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T05:43:22.982183Z","iopub.execute_input":"2025-03-19T05:43:22.982533Z","iopub.status.idle":"2025-03-19T07:28:14.175010Z","shell.execute_reply.started":"2025-03-19T05:43:22.982501Z","shell.execute_reply":"2025-03-19T07:28:14.174050Z"}},"outputs":[{"name":"stderr","text":"Processing metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228905/228905 [1:44:50<00:00, 36.39it/s]  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport os\nimport seaborn as sns\nfrom scipy.stats import fisher_exact\n\n# ChestXrayClassifier model definition (unchanged)\nclass ChestXrayClassifier(nn.Module):\n    def __init__(self, input_dim=1376, hidden_dims=[512, 384, 256], output_dim=14):\n        super(ChestXrayClassifier, self).__init__()\n        \n        # Input normalization layer (learnable)\n        self.batch_norm_input = nn.BatchNorm1d(input_dim)\n        \n        # Main network with residual connections\n        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n        self.dropout1 = nn.Dropout(0.3)\n        \n        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n        self.dropout2 = nn.Dropout(0.3)\n        \n        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n        self.dropout3 = nn.Dropout(0.3)\n        \n        # Residual connection (from input to hidden layer 2)\n        self.res_fc1 = nn.Linear(input_dim, hidden_dims[1])\n        \n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dims[2], hidden_dims[2] // 4),\n            nn.ReLU(),\n            nn.Linear(hidden_dims[2] // 4, hidden_dims[2]),\n            nn.Sigmoid()\n        )\n        \n        # Disease-specific layers for each output class\n        self.disease_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dims[2], hidden_dims[2] // 2),\n                nn.ReLU(),\n                nn.Linear(hidden_dims[2] // 2, 1)\n            ) for _ in range(output_dim)\n        ])\n    \n    def forward(self, x):\n        # Input normalization\n        x_norm = self.batch_norm_input(x)\n        \n        # First block with residual connection\n        res = x_norm\n        x = self.fc1(x_norm)\n        x = self.bn1(x)\n        x = nn.functional.leaky_relu(x, 0.1)\n        x = self.dropout1(x)\n        \n        # Residual connection from input to second layer\n        res = self.res_fc1(res)\n        \n        # Second block\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = nn.functional.leaky_relu(x, 0.1)\n        x = self.dropout2(x)\n        \n        # Add residual connection\n        x = x + res\n        \n        # Third block\n        x = self.fc3(x)\n        x = self.bn3(x)\n        x = nn.functional.leaky_relu(x, 0.1)\n        x = self.dropout3(x)\n        \n        # Apply attention\n        attention_weights = self.attention(x)\n        x = x * attention_weights\n        \n        # Disease-specific predictions\n        outputs = []\n        for disease_layer in self.disease_layers:\n            outputs.append(disease_layer(x))\n        \n        # Concatenate all outputs\n        return torch.cat(outputs, dim=1)\n\n# Modified function to save predictions with demographic information\ndef save_predictions_to_csv(dataset_loader, model, device, label_columns, file_name):\n    \"\"\"\n    Generate and save model predictions\n    \n    Args:\n        dataset_loader: DataLoader with the dataset\n        model: Trained model to generate predictions\n        device: Device to run model on\n        label_columns: List of disease label names\n        file_name: Where to save the CSV\n    \n    Returns:\n        all_probs: Array of prediction probabilities\n        all_targets: Array of ground truth labels\n    \"\"\"\n    model.eval()\n    all_outputs = []\n    all_targets = []\n    subject_ids = []\n    study_ids = []\n    splits = []\n    demo_data = {\n        'gender': [],\n        'race': [],\n        'insurance': [],\n        'anchor_age': []\n    }\n    \n    with torch.no_grad():\n        for batch in tqdm(dataset_loader, desc=\"Generating predictions\"):\n            inputs = batch['embedding'].to(device)\n            targets = batch['labels'].to(device)\n            \n            # Extract IDs and split\n            subject_ids.extend(batch['subject_id'])\n            study_ids.extend(batch['study_id'])\n            splits.extend(batch['split'])\n            \n            # Extract demographic info\n            for key in demo_data:\n                if key in batch:\n                    demo_data[key].extend(batch[key])\n            \n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n    \n    all_outputs = np.vstack(all_outputs)\n    all_targets = np.vstack(all_targets)\n    all_probs = 1 / (1 + np.exp(-all_outputs))  # sigmoid\n    all_binary_preds = (all_probs >= 0.5).astype(int)  # Convert to binary predictions\n    \n    # Create DataFrame for predictions\n    pred_df = pd.DataFrame()\n    pred_df['subject_id'] = subject_ids\n    pred_df['study_id'] = study_ids\n    pred_df['split'] = splits\n    \n    # Add demographics that we have\n    for key, values in demo_data.items():\n        if values:  # Only add if we have values\n            pred_df[key] = values\n    \n    # Add true labels and predictions for each disease\n    for i, label in enumerate(label_columns):\n        pred_df[f\"{label}_true\"] = all_targets[:, i]\n        pred_df[f\"{label}\"] = all_binary_preds[:, i]  # Binary predictions\n        pred_df[f\"{label}_prob\"] = all_probs[:, i]    # Probabilities\n    \n    # Save to CSV\n    pred_df.to_csv(file_name, index=False)\n    print(f\"Predictions saved to {file_name}\")\n    \n    return all_probs, all_targets, pred_df\n\n# Function to analyze predictions by demographic subgroups\ndef analyze_by_subgroups(predictions_df, label_columns, demographic_cols=None):\n    \"\"\"\n    Analyze model performance across different demographic subgroups\n    \n    Args:\n        predictions_df: DataFrame with predictions and demographic info\n        label_columns: List of disease labels\n        demographic_cols: List of demographic columns to analyze by\n                      (default: ['gender', 'race', 'insurance', 'anchor_age'])\n    \n    Returns:\n        results_dict: Dictionary with performance metrics by subgroup\n    \"\"\"\n    if demographic_cols is None:\n        demographic_cols = ['gender', 'race', 'insurance', 'anchor_age']\n    \n    results_dict = {}\n    \n    # Analyze each subgroup separately\n    for group_col in demographic_cols:\n        if group_col not in predictions_df.columns:\n            continue\n            \n        # Get unique values for this subgroup\n        subgroups = predictions_df[group_col].dropna().unique()\n        \n        group_results = {}\n        for subgroup in subgroups:\n            # Filter for this subgroup\n            subgroup_df = predictions_df[predictions_df[group_col] == subgroup]\n            \n            if len(subgroup_df) < 10:  # Skip if too few samples\n                continue\n                \n            # Calculate metrics for each disease label\n            disease_metrics = {}\n            for label in label_columns:\n                true_col = f\"{label}_true\"\n                pred_col = f\"{label}\"\n                prob_col = f\"{label}_prob\"\n                \n                # Skip if column doesn't exist\n                if true_col not in subgroup_df.columns or pred_col not in subgroup_df.columns:\n                    continue\n                \n                # Extract ground truth and predictions\n                y_true = subgroup_df[true_col].values\n                y_pred = subgroup_df[pred_col].values\n                y_prob = subgroup_df[prob_col].values if prob_col in subgroup_df.columns else None\n                \n                # Skip if no positive examples\n                if sum(y_true) == 0:\n                    continue\n                    \n                # Calculate metrics\n                accuracy = accuracy_score(y_true, y_pred)\n                precision = precision_score(y_true, y_pred, zero_division=0)\n                recall = recall_score(y_true, y_pred, zero_division=0)\n                f1 = f1_score(y_true, y_pred, zero_division=0)\n                \n                # Calculate AUC if we have probabilities\n                auc = None\n                if y_prob is not None:\n                    try:\n                        auc = roc_auc_score(y_true, y_prob)\n                    except:\n                        pass\n                \n                # Confusion matrix elements for TPR/FPR analysis\n                tn = sum((y_true == 0) & (y_pred == 0))\n                fp = sum((y_true == 0) & (y_pred == 1))\n                fn = sum((y_true == 1) & (y_pred == 0))\n                tp = sum((y_true == 1) & (y_pred == 1))\n                \n                # Calculate TPR and FPR\n                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n                \n                # Store results\n                disease_metrics[label] = {\n                    'accuracy': accuracy,\n                    'precision': precision,\n                    'recall': recall,\n                    'f1': f1,\n                    'auc': auc,\n                    'tpr': tpr,\n                    'fpr': fpr,\n                    'true_positives': tp,\n                    'false_positives': fp,\n                    'true_negatives': tn,\n                    'false_negatives': fn,\n                    'sample_count': len(y_true),\n                    'positive_count': sum(y_true)\n                }\n            \n            group_results[subgroup] = {\n                'sample_count': len(subgroup_df),\n                'disease_metrics': disease_metrics\n            }\n        \n        results_dict[group_col] = group_results\n    \n    return results_dict\n\n# Function to create visualizations of subgroup performance\ndef visualize_subgroup_performance(subgroup_results, metric='f1', output_dir='subgroup_analysis'):\n    \"\"\"\n    Create visualizations comparing model performance across subgroups\n    \n    Args:\n        subgroup_results: Results dictionary from analyze_by_subgroups\n        metric: Which metric to visualize ('accuracy', 'precision', 'recall', 'f1', 'tpr', 'fpr')\n        output_dir: Directory to save plots\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    for group_name, group_data in subgroup_results.items():\n        # Get all diseases across all subgroups\n        all_diseases = set()\n        for subgroup, subgroup_data in group_data.items():\n            all_diseases.update(subgroup_data['disease_metrics'].keys())\n        \n        # Create a dataframe for plotting\n        plot_data = []\n        for subgroup, subgroup_data in group_data.items():\n            for disease, metrics in subgroup_data['disease_metrics'].items():\n                if metric in metrics:\n                    plot_data.append({\n                        'Subgroup': subgroup,\n                        'Disease': disease,\n                        metric: metrics[metric],\n                        'Sample Count': metrics['sample_count']\n                    })\n        \n        if not plot_data:\n            continue\n            \n        plot_df = pd.DataFrame(plot_data)\n        \n        # Create heatmap\n        plt.figure(figsize=(12, 8))\n        pivot_table = plot_df.pivot_table(\n            values=metric, \n            index='Disease', \n            columns='Subgroup'\n        )\n        \n        sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=.5)\n        plt.title(f'{metric.capitalize()} by {group_name} Subgroups')\n        plt.tight_layout()\n        plt.savefig(f\"{output_dir}/{group_name}_{metric}_heatmap.png\")\n        plt.close()\n        \n        # Create grouped bar plot\n        plt.figure(figsize=(14, 10))\n        sns.barplot(x='Disease', y=metric, hue='Subgroup', data=plot_df)\n        plt.title(f'{metric.capitalize()} by Disease and {group_name}')\n        plt.xticks(rotation=45, ha='right')\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.tight_layout()\n        plt.savefig(f\"{output_dir}/{group_name}_{metric}_barplot.png\")\n        plt.close()\n\n# Function to analyze TPR and disparities by subgroup\n# Function to analyze TPR fairness (fixed)\ndef analyze_tpr_fairness(predictions_df, label_columns, demographic_cols=None, significance_threshold=0.05):\n    \"\"\"\n    Perform statistical analysis on TPR disparities to identify significant differences\n    \n    Args:\n        predictions_df: DataFrame with predictions and demographic info\n        label_columns: List of disease labels\n        demographic_cols: List of demographic columns to analyze by\n        significance_threshold: p-value threshold for statistical significance\n        \n    Returns:\n        DataFrame with TPR disparities by demographic group\n    \"\"\"\n    from scipy.stats import fisher_exact\n    \n    if demographic_cols is None:\n        demographic_cols = ['gender', 'race', 'insurance', 'anchor_age']\n    \n    demographic_cols = [g for g in demographic_cols if g in predictions_df.columns]\n    \n    result_data = []\n    \n    for group in demographic_cols:\n        # Get the majority subgroup as reference\n        if group not in predictions_df.columns or predictions_df[group].isnull().all():\n            continue\n            \n        reference = predictions_df[group].value_counts().index[0]\n        \n        for label in label_columns:\n            true_col = f\"{label}_true\"\n            pred_col = f\"{label}\"\n            \n            # Skip if columns don't exist\n            if true_col not in predictions_df.columns or pred_col not in predictions_df.columns:\n                continue\n            \n            # Get reference subgroup contingency table \n            ref_df = predictions_df[predictions_df[group] == reference]\n            ref_tp = np.sum((ref_df[true_col] == 1) & (ref_df[pred_col] == 1))\n            ref_fn = np.sum((ref_df[true_col] == 1) & (ref_df[pred_col] == 0))\n            ref_tpr = ref_tp / (ref_tp + ref_fn) if (ref_tp + ref_fn) > 0 else 0\n            \n            # Compare with other subgroups\n            for subgroup in predictions_df[group].dropna().unique():\n                if subgroup == reference:\n                    continue\n                \n                subgroup_df = predictions_df[predictions_df[group] == subgroup]\n                \n                # Skip if too few samples\n                if len(subgroup_df) < 20:\n                    continue\n                \n                # Calculate TPR\n                sg_tp = np.sum((subgroup_df[true_col] == 1) & (subgroup_df[pred_col] == 1))\n                sg_fn = np.sum((subgroup_df[true_col] == 1) & (subgroup_df[pred_col] == 0))\n                sg_tpr = sg_tp / (sg_tp + sg_fn) if (sg_tp + sg_fn) > 0 else 0\n                \n                # Skip if no positive cases\n                if (ref_tp + ref_fn) == 0 or (sg_tp + sg_fn) == 0:\n                    continue\n                \n                # Statistical test (Fisher's exact test on the contingency table)\n                contingency = np.array([[ref_tp, ref_fn], [sg_tp, sg_fn]])\n                \n                try:\n                    odds_ratio, p_value = fisher_exact(contingency)\n                    significant = p_value < significance_threshold\n                except:\n                    # If statistical test fails, skip\n                    continue\n                \n                tpr_disparity = sg_tpr - ref_tpr\n                \n                result_data.append({\n                    'Demographic_Group': group,\n                    'Reference': reference,\n                    'Subgroup': subgroup,\n                    'Disease': label,\n                    'Reference_TPR': ref_tpr,\n                    'Subgroup_TPR': sg_tpr,\n                    'TPR_Disparity': tpr_disparity,\n                    'P_Value': p_value,\n                    'Significant': significant,\n                    'Reference_Positive_Count': ref_tp + ref_fn,\n                    'Subgroup_Positive_Count': sg_tp + sg_fn\n                })\n    \n    # Create DataFrame and ensure 'Significant' column exists\n    if not result_data:\n        # Return empty DataFrame with all required columns\n        return pd.DataFrame(columns=[\n            'Demographic_Group', 'Reference', 'Subgroup', 'Disease',\n            'Reference_TPR', 'Subgroup_TPR', 'TPR_Disparity', 'P_Value',\n            'Significant', 'Reference_Positive_Count', 'Subgroup_Positive_Count'\n        ])\n    \n    result_df = pd.DataFrame(result_data)\n    \n    # Ensure Significant column exists (should already be there but just to be safe)\n    if 'Significant' not in result_df.columns:\n        result_df['Significant'] = result_df['P_Value'] < significance_threshold\n    \n    return result_df\n                \n\n\n# Function to plot TPR and disparities by subgroup\ndef plot_subgroup_tpr_and_disparities(predictions_df, label_columns, output_dir='tpr_analysis'):\n    \"\"\"\n    Create plots showing:\n    1. TPR (recall) for each subgroup within each demographic group\n    2. Disparity from overall TPR for each subgroup\n    \n    Args:\n        predictions_df: DataFrame with predictions and demographic data\n        label_columns: List of disease labels\n        output_dir: Directory to save output plots\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define demographic groups to analyze\n    demographic_groups = ['gender', 'race', 'insurance', 'anchor_age']\n    demographic_groups = [g for g in demographic_groups if g in predictions_df.columns]\n    \n    # First, calculate overall TPR for each disease\n    overall_tpr = {}\n    for label in label_columns:\n        true_col = f\"{label}_true\"\n        pred_col = f\"{label}\"\n        \n        # Skip if columns don't exist\n        if true_col not in predictions_df.columns or pred_col not in predictions_df.columns:\n            continue\n        \n        # Calculate overall TPR (recall)\n        y_true = predictions_df[true_col].values\n        y_pred = predictions_df[pred_col].values\n        \n        # Skip if no positive cases\n        if sum(y_true) == 0:\n            continue\n            \n        overall_tpr[label] = recall_score(y_true, y_pred, zero_division=0)\n    \n    # Now calculate TPR for each subgroup and the disparity\n    tpr_data = []\n    \n    for group in demographic_groups:\n        for label in overall_tpr.keys():\n            true_col = f\"{label}_true\"\n            pred_col = f\"{label}\"\n            \n            # Calculate TPR for each subgroup in this demographic group\n            for subgroup in predictions_df[group].dropna().unique():\n                subgroup_df = predictions_df[predictions_df[group] == subgroup]\n                \n                # Skip if too few samples\n                if len(subgroup_df) < 10:\n                    continue\n                \n                y_true = subgroup_df[true_col].values\n                y_pred = subgroup_df[pred_col].values\n                \n                # Skip if no positive cases in this subgroup\n                if sum(y_true) == 0:\n                    continue\n                \n                # Calculate TPR for this subgroup\n                subgroup_tpr = recall_score(y_true, y_pred, zero_division=0)\n                \n                # Calculate disparity from overall TPR\n                tpr_disparity = subgroup_tpr - overall_tpr[label]\n                \n                # Add to data collection\n                tpr_data.append({\n                    'Demographic_Group': group,\n                    'Subgroup': subgroup,\n                    'Disease': label,\n                    'TPR': subgroup_tpr,\n                    'Overall_TPR': overall_tpr[label],\n                    'TPR_Disparity': tpr_disparity,\n                    'Sample_Count': len(y_true),\n                    'Positive_Count': sum(y_true)\n                })\n    \n    # Convert to DataFrame\n    tpr_df = pd.DataFrame(tpr_data)\n    \n    # Save the data\n    tpr_df.to_csv(f\"{output_dir}/tpr_by_subgroup.csv\", index=False)\n    \n    # Create plots for each demographic group\n    for group in demographic_groups:\n        group_data = tpr_df[tpr_df['Demographic_Group'] == group]\n        \n        if len(group_data) == 0:\n            continue\n        \n        # Plot 1: TPR by subgroup for each disease\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Disease', y='TPR', hue='Subgroup', data=group_data)\n        plt.title(f'True Positive Rate by {group} Subgroup')\n        plt.xticks(rotation=45, ha='right')\n        plt.ylim(0, 1)  # TPR ranges from 0 to 1\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.tight_layout()\n        plt.savefig(f\"{output_dir}/{group}_tpr_by_subgroup.png\")\n        plt.close()\n        \n        # Plot 2: TPR Disparity from overall TPR\n        plt.figure(figsize=(14, 8))\n        sns.barplot(x='Disease', y='TPR_Disparity', hue='Subgroup', data=group_data)\n        plt.title(f'TPR Disparity from Overall by {group} Subgroup')\n        plt.xticks(rotation=45, ha='right')\n        plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)  # Add a line at y=0\n        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.tight_layout()\n        plt.savefig(f\"{output_dir}/{group}_tpr_disparity.png\")\n        plt.close()\n        \n        # Plot 3: Heatmap of TPR by subgroup and disease\n        if len(group_data['Subgroup'].unique()) > 1 and len(group_data['Disease'].unique()) > 1:\n            plt.figure(figsize=(12, 8))\n            pivot_tpr = group_data.pivot_table(\n                values='TPR', \n                index='Disease', \n                columns='Subgroup'\n            )\n            sns.heatmap(pivot_tpr, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=.5)\n            plt.title(f'TPR Heatmap by {group} Subgroup')\n            plt.tight_layout()\n            plt.savefig(f\"{output_dir}/{group}_tpr_heatmap.png\")\n            plt.close()\n            \n            # Plot 4: Heatmap of TPR disparities\n            plt.figure(figsize=(12, 8))\n            pivot_disparity = group_data.pivot_table(\n                values='TPR_Disparity', \n                index='Disease', \n                columns='Subgroup'\n            )\n            # Use a diverging colormap centered at 0\n            sns.heatmap(pivot_disparity, annot=True, cmap='RdBu_r', fmt='.2f', \n                        linewidths=.5, center=0)\n            plt.title(f'TPR Disparity Heatmap by {group} Subgroup')\n            plt.tight_layout()\n            plt.savefig(f\"{output_dir}/{group}_tpr_disparity_heatmap.png\")\n            plt.close()\n\n# Function to analyze fairness metrics across subgroups\ndef analyze_fairness_metrics(predictions_df, label_columns, demographic_cols=None):\n    \"\"\"\n    Calculate comprehensive fairness metrics across demographic groups\n    \n    Args:\n        predictions_df: DataFrame with predictions and demographic info\n        label_columns: List of disease labels\n        demographic_cols: List of demographic columns to analyze\n        \n    Returns:\n        DataFrame with fairness metrics\n    \"\"\"\n    if demographic_cols is None:\n        demographic_cols = ['gender', 'race', 'insurance', 'anchor_age']\n    \n    demographic_cols = [g for g in demographic_cols if g in predictions_df.columns]\n    \n    fairness_metrics = []\n    \n    for group in demographic_cols:\n        # Get subgroups\n        subgroups = predictions_df[group].dropna().unique()\n        \n        # Get the majority subgroup as reference\n        reference = predictions_df[group].value_counts().index[0]\n        \n        for label in label_columns:\n            true_col = f\"{label}_true\"\n            pred_col = f\"{label}\"\n            \n            # Skip if columns don't exist\n            if true_col not in predictions_df.columns or pred_col not in predictions_df.columns:\n                continue\n            \n            # Calculate overall metrics for this disease\n            y_true_all = predictions_df[true_col].values\n            y_pred_all = predictions_df[pred_col].values\n            \n            # Skip if no positive cases\n            if sum(y_true_all) == 0:\n                continue\n                \n            # Calculate TPR and FPR for each subgroup\n            for subgroup in subgroups:\n                subgroup_df = predictions_df[predictions_df[group] == subgroup]\n                \n                # Skip if too few samples\n                if len(subgroup_df) < 20:\n                    continue\n                \n                y_true = subgroup_df[true_col].values\n                y_pred = subgroup_df[pred_col].values\n                \n                # Skip if no positive or negative cases\n                if sum(y_true) == 0 or sum(y_true) == len(y_true):\n                    continue\n                \n                # Calculate metrics\n                accuracy = accuracy_score(y_true, y_pred)\n                precision = precision_score(y_true, y_pred, zero_division=0)\n                recall = recall_score(y_true, y_pred, zero_division=0)  # Same as TPR\n                f1 = f1_score(y_true, y_pred, zero_division=0)\n                \n                # Confusion matrix elements\n                tp = sum((y_true == 1) & (y_pred == 1))\n                fp = sum((y_true == 0) & (y_pred == 1))\n                tn = sum((y_true == 0) & (y_pred == 0))\n                fn = sum((y_true == 1) & (y_pred == 0))\n                \n                # Fairness metrics\n                tpr = recall  # True Positive Rate = Recall\n                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n                fnr = fn / (tp + fn) if (tp + fn) > 0 else 0  # False Negative Rate\n                tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n                \n                # Calculate prevalence (percentage of positive cases)\n                prevalence = sum(y_true) / len(y_true)\n                \n                # Disparate impact = (% predicted positive for protected group) / (% predicted positive for reference group)\n                # We'll calculate this at group level after collecting data for all subgroups\n                predicted_positive_rate = sum(y_pred) / len(y_pred)\n                \n                # Add to results\n                fairness_metrics.append({\n                    'Demographic_Group': group,\n                    'Subgroup': subgroup,\n                    'Is_Reference': subgroup == reference,\n                    'Disease': label,\n                    'Accuracy': accuracy,\n                    'Precision': precision,\n                    'Recall': recall,\n                    'F1': f1,\n                    'TPR': tpr,\n                    'FPR': fpr,\n                    'TNR': tnr,\n                    'FNR': fnr,\n                    'Prevalence': prevalence,\n                    'Predicted_Positive_Rate': predicted_positive_rate,\n                    'Sample_Count': len(y_true),\n                    'Positive_Count': sum(y_true),\n                    'Predicted_Positive_Count': sum(y_pred)\n                })\n    \n    fairness_df = pd.DataFrame(fairness_metrics)\n    \n    # Calculate disparate impact and equalized odds for each (demographic_group, disease) pair\n    disparity_results = []\n    \n    for group in demographic_cols:\n        for label in label_columns:\n            group_disease_df = fairness_df[(fairness_df['Demographic_Group'] == group) & \n                                           (fairness_df['Disease'] == label)]\n            \n            if len(group_disease_df) <= 1:  # Need at least two subgroups to compare\n                continue\n                \n            reference_row = group_disease_df[group_disease_df['Is_Reference'] == True]\n            if len(reference_row) == 0:\n                continue\n                \n            reference_tpr = reference_row['TPR'].values[0]\n            reference_fpr = reference_row['FPR'].values[0]\n            reference_ppr = reference_row['Predicted_Positive_Rate'].values[0]\n            \n            for idx, row in group_disease_df[group_disease_df['Is_Reference'] == False].iterrows():\n                # Calculate disparity metrics\n                tpr_disparity = row['TPR'] - reference_tpr\n                fpr_disparity = row['FPR'] - reference_fpr\n                \n                # Disparate impact: ratio of predicted positive rates\n                disparate_impact = row['Predicted_Positive_Rate'] / reference_ppr if reference_ppr > 0 else 0\n                \n                # Equal opportunity difference (difference in TPR)\n                equal_opportunity_diff = tpr_disparity\n                \n                # Equalized odds violation (max of abs differences in TPR and FPR)\n                equalized_odds_violation = max(abs(tpr_disparity), abs(fpr_disparity))\n                \n                disparity_results.append({\n                    'Demographic_Group': group,\n                    'Reference': reference_row['Subgroup'].values[0],\n                    'Subgroup': row['Subgroup'],\n                    'Disease': label,\n                    'TPR_Disparity': tpr_disparity,\n                    'FPR_Disparity': fpr_disparity,\n                    'Disparate_Impact': disparate_impact,\n                    'Equal_Opportunity_Diff': equal_opportunity_diff,\n                    'Equalized_Odds_Violation': equalized_odds_violation,\n                    'Reference_Sample_Count': reference_row['Sample_Count'].values[0],\n                    'Subgroup_Sample_Count': row['Sample_Count']\n                })\n    \n    disparity_df = pd.DataFrame(disparity_results)\n    \n    return fairness_df, disparity_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:28:14.176644Z","iopub.execute_input":"2025-03-19T07:28:14.176890Z","iopub.status.idle":"2025-03-19T07:28:15.309242Z","shell.execute_reply.started":"2025-03-19T07:28:14.176869Z","shell.execute_reply":"2025-03-19T07:28:15.308563Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Main training function with fairness analysis\ndef train_and_evaluate(train_loader, val_loader, test_loader, label_columns, num_epochs=25):\n    \"\"\"\n    Complete pipeline for training, evaluation, and subgroup analysis\n    \n    Args:\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        test_loader: DataLoader for test data\n        label_columns: List of disease labels\n        num_epochs: Number of training epochs\n    \"\"\"\n    # Create output directories\n    os.makedirs('predictions', exist_ok=True)\n    os.makedirs('subgroup_analysis', exist_ok=True)\n    os.makedirs('tpr_analysis', exist_ok=True)\n    os.makedirs('fairness_analysis', exist_ok=True)\n    \n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Initialize model\n    model = ChestXrayClassifier(input_dim=1376, output_dim=len(label_columns))\n    model.to(device)\n    \n    # Initialize loss and optimizer\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n    \n    # Training setup\n    train_losses = []\n    val_losses = []\n    val_aucs = []\n    best_val_auc = 0\n    best_model_path = 'best_model.pth'\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for batch in progress_bar:\n            # Move data to device\n            inputs = batch['embedding'].to(device)\n            targets = batch['labels'].to(device)\n            \n            # Zero the gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Update statistics\n            train_loss += loss.item() * inputs.size(0)\n            progress_bar.set_postfix({'loss': loss.item()})\n        \n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        all_outputs = []\n        all_targets = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = batch['embedding'].to(device)\n                targets = batch['labels'].to(device)\n                \n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                \n                val_loss += loss.item() * inputs.size(0)\n                all_outputs.append(outputs.cpu().numpy())\n                all_targets.append(targets.cpu().numpy())\n        \n        val_loss /= len(val_loader.dataset)\n        val_losses.append(val_loss)\n        \n        # Calculate AUC for each label\n        all_outputs = np.vstack(all_outputs)\n        all_targets = np.vstack(all_targets)\n        all_probs = 1 / (1 + np.exp(-all_outputs))  # sigmoid\n        \n        aucs = {}\n        for i, label in enumerate(label_columns):\n            if sum(all_targets[:, i]) > 0:  # Only if there are positive examples\n                aucs[label] = roc_auc_score(all_targets[:, i], all_probs[:, i])\n        \n        mean_auc = np.mean(list(aucs.values()))\n        val_aucs.append(mean_auc)\n        \n        # Update learning rate based on validation loss\n        scheduler.step(val_loss)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        print(f\"  Mean AUC: {mean_auc:.4f}\")\n        \n        # Save best model\n        if mean_auc > best_val_auc:\n            best_val_auc = mean_auc\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"  Saved new best model with AUC: {mean_auc:.4f}\")\n        \n        # Save predictions at final epoch\n        if epoch == num_epochs - 1:\n            print(\"\\nGenerating final predictions...\")\n            \n            # Load best model\n            model.load_state_dict(torch.load(best_model_path))\n            \n            # Generate predictions\n            train_probs, train_targets, train_pred_df = save_predictions_to_csv(\n                train_loader, model, device, label_columns, \n                'predictions/train_predictions_final.csv'\n            )\n            \n            val_probs, val_targets, val_pred_df = save_predictions_to_csv(\n                val_loader, model, device, label_columns, \n                'predictions/val_predictions_final.csv'\n            )\n            \n            test_probs, test_targets, test_pred_df = save_predictions_to_csv(\n                test_loader, model, device, label_columns, \n                'predictions/test_predictions_final.csv'\n            )\n            \n            # Perform TPR subgroup analysis on test predictions\n            print(\"\\nPerforming TPR subgroup analysis...\")\n            plot_subgroup_tpr_and_disparities(test_pred_df, label_columns)\n            \n            # Perform statistical analysis of TPR disparities\n            # Perform statistical analysis of TPR disparities\n            print(\"Analyzing TPR disparities across subgroups...\")\n            tpr_disparities = analyze_tpr_fairness(test_pred_df, label_columns)\n            tpr_disparities.to_csv('tpr_analysis/significant_tpr_disparities.csv', index=False)\n            \n            # Print significant disparities\n            significant_disparities = tpr_disparities[tpr_disparities['Significant']]\n            \n            # With this:\n            # Perform statistical analysis of TPR disparities\n            print(\"Analyzing TPR disparities across subgroups...\")\n            tpr_disparities = analyze_tpr_fairness(test_pred_df, label_columns)\n            tpr_disparities.to_csv('tpr_analysis/significant_tpr_disparities.csv', index=False)\n\n            # Print significant disparities\n            if not tpr_disparities.empty and 'Significant' in tpr_disparities.columns:\n                significant_disparities = tpr_disparities[tpr_disparities['Significant']]\n                if len(significant_disparities) > 0:\n                    print(\"\\nSignificant TPR disparities found:\")\n                    for _, row in significant_disparities.sort_values('TPR_Disparity', ascending=False).head(5).iterrows():\n                        print(f\"  {row['Disease']} - {row['Demographic_Group']}: {row['Reference']} vs {row['Subgroup']}\")\n                        print(f\"    TPR disparity: {row['TPR_Disparity']:.4f} (p-value: {row['P_Value']:.4f})\")\n            else:\n                print(\"No significant TPR disparities found or not enough data for analysis\")\n            # Perform comprehensive fairness analysis\n            print(\"\\nPerforming comprehensive fairness analysis...\")\n            fairness_metrics, disparity_metrics = analyze_fairness_metrics(test_pred_df, label_columns)\n            \n            # Save fairness metrics\n            fairness_metrics.to_csv('fairness_analysis/fairness_metrics.csv', index=False)\n            disparity_metrics.to_csv('fairness_analysis/disparity_metrics.csv', index=False)\n            \n            # Generate subgroup performance visualizations\n            print(\"Creating subgroup performance visualizations...\")\n            demographic_cols = ['gender', 'race', 'insurance', 'anchor_age']\n            demographic_cols = [g for g in demographic_cols if g in test_pred_df.columns]\n            \n            subgroup_results = analyze_by_subgroups(test_pred_df, label_columns, demographic_cols)\n            \n            for metric in ['f1', 'recall', 'precision', 'accuracy', 'tpr', 'fpr']:\n                visualize_subgroup_performance(subgroup_results, metric=metric)\n                \n            # Visualize fairness metrics\n            print(\"Creating fairness metric visualizations...\")\n            for fairness_metric in ['TPR_Disparity', 'FPR_Disparity', 'Equal_Opportunity_Diff', 'Equalized_Odds_Violation']:\n                if fairness_metric in disparity_metrics.columns:\n                    plt.figure(figsize=(12, 8))\n                    sns.boxplot(x='Disease', y=fairness_metric, hue='Demographic_Group', data=disparity_metrics)\n                    plt.title(f'{fairness_metric} by Disease and Demographic Group')\n                    plt.xticks(rotation=45, ha='right')\n                    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n                    plt.tight_layout()\n                    plt.savefig(f\"fairness_analysis/{fairness_metric}_boxplot.png\")\n                    plt.close()\n    \n    # Plot training curves\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(val_aucs, label='Validation AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('AUC')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig('training_curves.png')\n    plt.show()\n    \n    # Final evaluation\n    model.load_state_dict(torch.load(best_model_path))\n    model.eval()\n    test_outputs = []\n    test_targets_list = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = batch['embedding'].to(device)\n            targets = batch['labels'].to(device)\n            \n            outputs = model(inputs)\n            test_outputs.append(outputs.cpu().numpy())\n            test_targets_list.append(targets.cpu().numpy())\n    \n    test_outputs = np.vstack(test_outputs)\n    test_targets = np.vstack(test_targets_list)\n    test_probs = 1 / (1 + np.exp(-test_outputs))\n    test_preds = (test_probs >= 0.5).astype(int)\n    \n    # Calculate final metrics\n    metrics_df = pd.DataFrame(columns=['Label', 'AUC', 'Accuracy', 'Precision', 'Recall', 'F1'])\n    \n    print(\"\\nFinal metrics by condition:\")\n    for i, label in enumerate(label_columns):\n        # Skip if no positive examples\n        if sum(test_targets[:, i]) > 0:\n            auc = roc_auc_score(test_targets[:, i], test_probs[:, i])\n            accuracy = accuracy_score(test_targets[:, i], test_preds[:, i])\n            precision = precision_score(test_targets[:, i], test_preds[:, i], zero_division=0)\n            recall = recall_score(test_targets[:, i], test_preds[:, i], zero_division=0)\n            f1 = f1_score(test_targets[:, i], test_preds[:, i], zero_division=0)\n            \n            print(f\"{label}:\")\n            print(f\"  AUC: {auc:.4f}\")\n            print(f\"  Accuracy: {accuracy:.4f}\")\n            print(f\"  Precision: {precision:.4f}\")\n            print(f\"  Recall: {recall:.4f}\")\n            print(f\"  F1: {f1:.4f}\")\n            \n            # Add to metrics dataframe\n            metrics_df = pd.concat([\n                metrics_df, \n                pd.DataFrame({\n                    'Label': [label],\n                    'AUC': [auc],\n                    'Accuracy': [accuracy],\n                    'Precision': [precision],\n                    'Recall': [recall],\n                    'F1': [f1]\n                })\n            ], ignore_index=True)\n    \n    print(f\"\\nOverall Mean AUC: {metrics_df['AUC'].mean():.4f}\")\n    metrics_df.to_csv('predictions/metrics_summary.csv', index=False)\n    \n    # Print fairness summary\n    print(\"\\nFairness Analysis Summary:\")\n    \n    # Get the largest disparities\n    if len(disparity_metrics) > 0:\n        worst_disparities = disparity_metrics.nlargest(5, 'TPR_Disparity')\n        print(\"\\nLargest TPR Disparities (Recall differences):\")\n        for _, row in worst_disparities.iterrows():\n            print(f\"  {row['Disease']} - {row['Demographic_Group']}: {row['Reference']} vs {row['Subgroup']}\")\n            print(f\"    TPR disparity: {row['TPR_Disparity']:.4f}\")\n        \n        if 'Equalized_Odds_Violation' in disparity_metrics.columns:\n            worst_eq_odds = disparity_metrics.nlargest(5, 'Equalized_Odds_Violation')\n            print(\"\\nLargest Equalized Odds Violations:\")\n            for _, row in worst_eq_odds.iterrows():\n                print(f\"  {row['Disease']} - {row['Demographic_Group']}: {row['Reference']} vs {row['Subgroup']}\")\n                print(f\"    Violation: {row['Equalized_Odds_Violation']:.4f}\")\n    \n    return model, metrics_df, fairness_metrics, disparity_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:28:15.310194Z","iopub.execute_input":"2025-03-19T07:28:15.310623Z","iopub.status.idle":"2025-03-19T07:28:15.334851Z","shell.execute_reply.started":"2025-03-19T07:28:15.310601Z","shell.execute_reply":"2025-03-19T07:28:15.333891Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# Define label columns\nlabel_columns = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n                'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', \n                'Atelectasis', 'Pneumothorax', 'Pleural Effusion', \n                'Pleural Other', 'Fracture', 'Support Devices', 'No Finding']\n\n# Train and evaluate the model with fairness analysis\nmodel, metrics_df, fairness_metrics, disparity_metrics = train_and_evaluate(\n    train_loader, val_loader, test_loader, label_columns, num_epochs=10\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:28:15.335609Z","iopub.execute_input":"2025-03-19T07:28:15.335821Z","iopub.status.idle":"2025-03-19T07:32:10.610909Z","shell.execute_reply.started":"2025-03-19T07:28:15.335803Z","shell.execute_reply":"2025-03-19T07:32:10.609799Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f77ea7b897f04e1eb213b153447caff2"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10:\n  Train Loss: 0.2585\n  Val Loss: 0.2507\n  Mean AUC: 0.8184\n  Saved new best model with AUC: 0.8184\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558329fafd864eb2b7e6c4cc8fff793a"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/10:\n  Train Loss: 0.2514\n  Val Loss: 0.2487\n  Mean AUC: 0.8242\n  Saved new best model with AUC: 0.8242\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d13c270d7a84e82b36dfb7c8409b799"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/10:\n  Train Loss: 0.2495\n  Val Loss: 0.2484\n  Mean AUC: 0.8260\n  Saved new best model with AUC: 0.8260\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b40c8e82f4d40cebc90100ba008e536"}},"metadata":{}},{"name":"stdout","text":"Epoch 4/10:\n  Train Loss: 0.2481\n  Val Loss: 0.2473\n  Mean AUC: 0.8291\n  Saved new best model with AUC: 0.8291\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a342972c48451dae9796c44470589e"}},"metadata":{}},{"name":"stdout","text":"Epoch 5/10:\n  Train Loss: 0.2470\n  Val Loss: 0.2474\n  Mean AUC: 0.8276\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f008a9448a4ec494d7ec9adaec7e06"}},"metadata":{}},{"name":"stdout","text":"Epoch 6/10:\n  Train Loss: 0.2459\n  Val Loss: 0.2464\n  Mean AUC: 0.8310\n  Saved new best model with AUC: 0.8310\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9feaf04eb29a4d89b9ac95c150342855"}},"metadata":{}},{"name":"stdout","text":"Epoch 7/10:\n  Train Loss: 0.2448\n  Val Loss: 0.2465\n  Mean AUC: 0.8316\n  Saved new best model with AUC: 0.8316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2343a27c018540379c6802d2c36c2485"}},"metadata":{}},{"name":"stdout","text":"Epoch 8/10:\n  Train Loss: 0.2440\n  Val Loss: 0.2458\n  Mean AUC: 0.8321\n  Saved new best model with AUC: 0.8321\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb617e69e574d13975cb8862061bf36"}},"metadata":{}},{"name":"stdout","text":"Epoch 9/10:\n  Train Loss: 0.2431\n  Val Loss: 0.2458\n  Mean AUC: 0.8336\n  Saved new best model with AUC: 0.8336\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b127ee09eedb4aa0b2850be899cd346c"}},"metadata":{}},{"name":"stdout","text":"Epoch 10/10:\n  Train Loss: 0.2422\n  Val Loss: 0.2460\n  Mean AUC: 0.8321\n\nGenerating final predictions...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-bb23d0b18378>:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating predictions:   0%|          | 0/1458 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde1f243ffa542a6af1cb7c77ab96fcd"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to predictions/train_predictions_final.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating predictions:   0%|          | 0/162 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0d7a3b0d664d44829c9740e311122b"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to predictions/val_predictions_final.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating predictions:   0%|          | 0/169 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b5b4ca787094a2989d6ebc6810cec07"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to predictions/test_predictions_final.csv\n\nPerforming TPR subgroup analysis...\nAnalyzing TPR disparities across subgroups...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c0f17010af23>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train and evaluate the model with fairness analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model, metrics_df, fairness_metrics, disparity_metrics = train_and_evaluate(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;32m<ipython-input-5-bb23d0b18378>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_loader, val_loader, test_loader, label_columns, num_epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Print significant disparities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msignificant_disparities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpr_disparities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtpr_disparities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Significant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignificant_disparities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSignificant TPR disparities found:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Significant'"],"ename":"KeyError","evalue":"'Significant'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport zipfile\n\n\nnew_zip_path = \"/kaggle/working/optimized_files.zip\"\nitems_to_zip = [\n    \"chest_xray_model.pth\",\n    \"predictions\",\n    \"subgroup_analysis\", \n    \"tpr_analysis\",\n    \"training_curves.png\"\n]\n\n# Use a memory-efficient approach\nwith zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    # Add files one by one\n    for item in items_to_zip:\n        item_path = os.path.join(\"/kaggle/working\", item)\n        \n        # Add file\n        if os.path.isfile(item_path):\n            zipf.write(item_path, arcname=item)\n            print(f\"Added file: {item}\")\n        \n        # Add directory contents\n        elif os.path.isdir(item_path):\n            # Get base name for path calculations\n            base_name = os.path.basename(item_path)\n            \n            # Walk through directory\n            for dir_path, _, files in os.walk(item_path):\n                # Skip empty directories\n                if not files:\n                    continue\n                    \n                # Process each file\n                for file in files:\n                    file_path = os.path.join(dir_path, file)\n                    # Calculate relative path for the archive\n                    arc_path = os.path.join(base_name, os.path.relpath(file_path, item_path))\n                    # Add to zip with correct path structure\n                    zipf.write(file_path, arcname=arc_path)\n\nprint(f\"Created optimized zip file: optimized_files.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:32:10.611473Z","iopub.status.idle":"2025-03-19T07:32:10.611729Z","shell.execute_reply":"2025-03-19T07:32:10.611624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}